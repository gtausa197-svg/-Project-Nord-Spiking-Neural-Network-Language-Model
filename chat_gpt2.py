"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   GPT-2 Small (124M) â€” Ð§Ð°Ñ‚ Ð´Ð»Ñ Ð¿Ð¾Ñ€Ñ–Ð²Ð½ÑÐ½Ð½Ñ Ð· Nord            â•‘
â•‘                                                              â•‘
â•‘   Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸:  python chat_gpt2.py                              â•‘
â•‘   ÐŸÐ¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾: pip install torch transformers                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import torch
import torch.nn.functional as F
import time

def main():
    print()
    print("â•" * 60)
    print("  ðŸ¤– GPT-2 Small (124M) â€” Baseline Chat")
    print("  Ð”Ð»Ñ Ð¿Ð¾Ñ€Ñ–Ð²Ð½ÑÐ½Ð½Ñ Ð· Nord SNN-LLM")
    print("â•" * 60)

    # Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ
    print("\n  [*] Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÑƒÑ”Ð¼Ð¾ GPT-2 Small (124M)...")
    from transformers import GPT2LMHeadModel, GPT2Tokenizer

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    model = GPT2LMHeadModel.from_pretrained("gpt2")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    model.eval()

    param_count = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"  [âœ“] Ð“Ð¾Ñ‚Ð¾Ð²Ð¾! ({param_count:.1f}M Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ–Ð², {device})")

    # ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ð½Ð½Ñ
    temperature = 0.8
    max_tokens = 200
    top_k = 50
    top_p = 0.9
    rep_penalty = 1.3

    print(f"\n  {'â”€' * 50}")
    print(f"  ÐŸÐ¸ÑˆÐ¸ Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½Ð½Ñ Ñ– Ð½Ð°Ñ‚Ð¸ÑÐºÐ°Ð¹ Enter.")
    print(f"  ÐšÐ¾Ð¼Ð°Ð½Ð´Ð¸:")
    print(f"    /quit          â€” Ð²Ð¸Ð¹Ñ‚Ð¸")
    print(f"    /temp 0.5      â€” Ð·Ð¼Ñ–Ð½Ð¸Ñ‚Ð¸ temperature")
    print(f"    /tokens 300    â€” Ð¼Ð°ÐºÑ. Ñ‚Ð¾ÐºÐµÐ½Ñ–Ð²")
    print(f"    /rep 1.5       â€” repetition penalty")
    print(f"  {'â”€' * 50}\n")

    while True:
        try:
            user_input = input("  Ð¢Ð¸: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\n  Ð‘ÑƒÐ²Ð°Ð¹! ðŸ‘‹")
            break

        if not user_input:
            continue

        # ÐšÐ¾Ð¼Ð°Ð½Ð´Ð¸
        if user_input.startswith("/"):
            parts = user_input.split()
            cmd = parts[0].lower()

            if cmd == "/quit":
                print("  Ð‘ÑƒÐ²Ð°Ð¹! ðŸ‘‹")
                break
            elif cmd == "/temp" and len(parts) > 1:
                try:
                    temperature = float(parts[1])
                    print(f"  [âš™] Temperature = {temperature}")
                except ValueError:
                    print(f"  [!] ÐÐµÐ²Ñ–Ñ€Ð½Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ")
            elif cmd == "/tokens" and len(parts) > 1:
                try:
                    max_tokens = int(parts[1])
                    print(f"  [âš™] Max tokens = {max_tokens}")
                except ValueError:
                    print(f"  [!] ÐÐµÐ²Ñ–Ñ€Ð½Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ")
            elif cmd == "/rep" and len(parts) > 1:
                try:
                    rep_penalty = float(parts[1])
                    print(f"  [âš™] Repetition penalty = {rep_penalty}")
                except ValueError:
                    print(f"  [!] ÐÐµÐ²Ñ–Ñ€Ð½Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð½Ñ")
            else:
                print(f"  [!] ÐÐµÐ²Ñ–Ð´Ð¾Ð¼Ð° ÐºÐ¾Ð¼Ð°Ð½Ð´Ð°: {cmd}")
            continue

        # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ
        t0 = time.time()

        input_ids = tokenizer.encode(user_input, return_tensors="pt").to(device)

        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=rep_penalty,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
            )

        elapsed = time.time() - t0

        # Ð”ÐµÐºÐ¾Ð´ÑƒÑ”Ð¼Ð¾ Ñ‚Ñ–Ð»ÑŒÐºÐ¸ Ð½Ð¾Ð²Ñ– Ñ‚Ð¾ÐºÐµÐ½Ð¸
        new_tokens = output[0][input_ids.shape[1]:]
        response = tokenizer.decode(new_tokens, skip_special_tokens=True)

        print(f"\n  GPT-2: {response}")

        resp_tokens = len(new_tokens)
        tps = resp_tokens / elapsed if elapsed > 0 else 0
        print(f"  [{resp_tokens} tok, {elapsed:.1f}s, {tps:.1f} tok/s]\n")


if __name__ == "__main__":
    main()
