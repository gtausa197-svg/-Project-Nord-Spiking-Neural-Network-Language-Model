"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         PROJECT NORD â€” ĞšÑ€Ğ¾Ğº 3: Ğ§Ğ°Ñ‚ Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ»Ñ  v3.1                     â•‘
â•‘                                                                        â•‘
â•‘  ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸:                                                       â•‘
â•‘      python chat.py                                                    â•‘
â•‘                                                                        â•‘
â•‘  Ğ’Ğ¾Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ñ” Ğ´Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ– Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ñ–Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹ Ñ‡Ğ°Ñ‚.           â•‘
â•‘  ĞŸÑ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ” STDP: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ¾Ğ²Ğ¸Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ñ€Ğ¾Ğ·Ğ¼Ğ¾Ğ²Ğ¸!    â•‘
â•‘  v3.1: Repetition Penalty â€” Ğ¼ĞµĞ½ÑˆĞµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½ÑŒ Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ—                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞŸĞ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾:
    pip install torch transformers
"""

from __future__ import annotations

import os
import sys
import time
from pathlib import Path
from collections import Counter

import torch
import torch.nn.functional as F

from nord_core import NordConfig, NordModel


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ—ĞĞ’ĞĞĞ¢ĞĞ–Ğ•ĞĞĞ¯ ĞœĞĞ”Ğ•Ğ›Ğ†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_model(model_dir: str) -> tuple:
    """Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ– Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€."""
    from transformers import AutoTokenizer

    model_path = Path(model_dir)

    # Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–
    candidates = ["nord_final.pt", "nord_latest.pt"]
    ckpt_path = None
    for name in candidates:
        p = model_path / name
        if p.exists():
            ckpt_path = p
            break

    if ckpt_path is None:
        steps = sorted(model_path.glob("nord_step_*.pt"))
        if steps:
            ckpt_path = steps[-1]

    if ckpt_path is None:
        print(f"  [âœ—] ĞĞµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ğ²: {model_dir}")
        print(f"  Ğ¡Ğ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ Ğ½Ğ°Ñ‚Ñ€ĞµĞ½ÑƒĞ¹:  python train_nord.py")
        sys.exit(1)

    print(f"  [*] Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ”Ğ¼Ğ¾: {ckpt_path.name}")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)

    saved_cfg = ckpt.get("config", {})
    cfg = NordConfig(
        device=device,
        dtype=torch.float16 if device == "cuda" else torch.float32,
        d_model=saved_cfg.get("d_model", 512),
        n_heads=saved_cfg.get("n_heads", 8),
        n_layers=saved_cfg.get("n_layers", 6),
        d_ff=saved_cfg.get("d_ff", 1024),
        T=saved_cfg.get("T", 8),
        T_slow=saved_cfg.get("T_slow", 2),
        max_seq_len=saved_cfg.get("max_seq_len", 512),
        vocab_size=saved_cfg.get("vocab_size", 128_256),
        persistent_mem=False,
    )

    model = NordModel(cfg).to(device)
    model.load_state_dict(ckpt["model_state_dict"])
    model.eval()

    print(f"  [*] Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ”Ğ¼Ğ¾ Llama-3.2 Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€...")
    tokenizer = AutoTokenizer.from_pretrained(
        cfg.tokenizer_id, trust_remote_code=True,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    param_count = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"  [âœ“] ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ°! ({param_count:.1f}M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ–Ğ²)")

    return model, tokenizer, cfg


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# REPETITION PENALTY
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def apply_repetition_penalty(
    logits: torch.Tensor,
    generated_ids: torch.Tensor,
    penalty: float = 1.3,
    window: int = 50,
) -> torch.Tensor:
    """
    Ğ—Ğ¼ĞµĞ½ÑˆÑƒÑ” Ğ¹Ğ¼Ğ¾Ğ²Ñ–Ñ€Ğ½Ñ–ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ² ÑĞºÑ– Ğ²Ğ¶Ğµ Ğ·'ÑĞ²Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ–Ñ… `window` Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ….
    penalty > 1.0 = Ğ·Ğ¼ĞµĞ½ÑˆÑƒÑ” Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ½Ñ (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ¾ 1.2-1.5)
    Ğ§Ğ¸Ğ¼ Ğ±Ñ–Ğ»ÑŒÑˆĞµ Ñ€Ğ°Ğ·Ñ–Ğ² Ñ‚Ğ¾ĞºĞµĞ½ Ğ·'ÑĞ²Ğ¸Ğ²ÑÑ â€” Ñ‚Ğ¸Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ–ÑˆĞ¸Ğ¹ penalty (Ğ´Ğ¾ 5x).
    """
    if penalty <= 1.0:
        return logits

    recent_ids = generated_ids[0, -window:].tolist()
    token_counts = Counter(recent_ids)

    for token_id, count in token_counts.items():
        if token_id < logits.size(-1):
            # Ğ•ĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ñ–Ğ¹Ğ½Ğ¸Ğ¹ penalty: penalty^min(count, 5)
            effective_penalty = penalty ** min(count, 5)
            if logits[0, token_id] > 0:
                logits[0, token_id] = logits[0, token_id] / effective_penalty
            else:
                logits[0, token_id] = logits[0, token_id] * effective_penalty

    return logits


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ“Ğ•ĞĞ•Ğ ĞĞ¦Ğ†Ğ¯ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@torch.no_grad()
def generate(
    model: NordModel,
    tokenizer,
    cfg: NordConfig,
    prompt: str,
    max_new_tokens: int = 200,
    temperature: float = 0.8,
    top_k: int = 50,
    top_p: float = 0.9,
    enable_stdp: bool = True,
    repetition_penalty: float = 1.3,
    rep_window: int = 50,
) -> str:
    """
    ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑĞ¸Ğ²Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ Ğ· SNN.
    v3.1: + repetition penalty Ğ´Ğ»Ñ Ñ€Ñ–Ğ·Ğ½Ğ¾Ğ¼Ğ°Ğ½Ñ–Ñ‚Ğ½Ñ–ÑˆĞ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ.
    """
    device = cfg.device

    model.reset_state()

    max_prompt_len = max(32, cfg.max_seq_len - max_new_tokens)
    enc = tokenizer(prompt, return_tensors="pt", truncation=True,
                    max_length=max_prompt_len)
    input_ids = enc.input_ids.to(device)
    generated_ids = input_ids.clone()

    for _ in range(max_new_tokens):
        context = generated_ids[:, -cfg.max_seq_len:]

        with torch.amp.autocast("cuda", enabled=(device == "cuda")):
            logits, stats = model(context, enable_stdp=enable_stdp)

        next_logits = logits[:, -1, :].float()

        # â”€â”€ Repetition Penalty (Ğ´Ğ¾ temperature!) â”€â”€
        next_logits = apply_repetition_penalty(
            next_logits, generated_ids,
            penalty=repetition_penalty,
            window=rep_window,
        )

        if temperature > 0:
            next_logits = next_logits / temperature

        if top_k > 0:
            top_k_vals, _ = torch.topk(next_logits, min(top_k, next_logits.size(-1)))
            threshold = top_k_vals[:, -1].unsqueeze(-1)
            next_logits[next_logits < threshold] = float("-inf")

        if top_p < 1.0:
            sorted_logits, sorted_idx = torch.sort(next_logits, descending=True)
            cumprobs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            remove_mask = cumprobs - F.softmax(sorted_logits, dim=-1) > top_p
            sorted_logits[remove_mask] = float("-inf")
            next_logits.scatter_(1, sorted_idx, sorted_logits)

        probs = F.softmax(next_logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        generated_ids = torch.cat([generated_ids, next_token], dim=-1)

        # v3: Reward-modulated STDP
        if enable_stdp:
            loss_proxy = -torch.log(probs.max() + 1e-8).item()
            model.stdp_update(current_loss=loss_proxy)

        if next_token.item() == tokenizer.eos_token_id:
            break

    new_ids = generated_ids[0, input_ids.shape[1]:]
    return tokenizer.decode(new_ids, skip_special_tokens=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ†ĞĞ¢Ğ•Ğ ĞĞšĞ¢Ğ˜Ğ’ĞĞ˜Ğ™ Ğ§ĞĞ¢
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chat_loop(model: NordModel, tokenizer, cfg: NordConfig):
    """Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¸Ğ¹ Ñ†Ğ¸ĞºĞ» Ñ‡Ğ°Ñ‚Ñƒ."""

    temperature = 0.8
    max_tokens = 200
    stdp_enabled = True
    rep_penalty = 1.3
    rep_window = 50

    print(f"\n  {'â”€' * 50}")
    print(f"  ĞŸĞ¸ÑˆĞ¸ Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ Ñ– Ğ½Ğ°Ñ‚Ğ¸ÑĞºĞ°Ğ¹ Enter.")
    print(f"  ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¸:")
    print(f"    /quit          â€” Ğ²Ğ¸Ğ¹Ñ‚Ğ¸")
    print(f"    /temp 0.5      â€” Ğ·Ğ¼Ñ–Ğ½Ğ¸Ñ‚Ğ¸ temperature")
    print(f"    /tokens 300    â€” Ğ¼Ğ°ĞºÑ. Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ² Ñƒ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–")
    print(f"    /stdp on|off   â€” STDP Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ñ‡Ğ°Ñ‚Ñƒ")
    print(f"    /rep 1.5       â€” repetition penalty (1.0=Ğ²Ğ¸Ğ¼Ğº, 1.2-1.5=Ğ½Ğ¾Ñ€Ğ¼)")
    print(f"    /stats         â€” Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚Ğ¸ ÑĞ¿Ğ°Ğ¹Ğº-ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ")
    print(f"    /reset         â€” ÑĞºĞ¸Ğ½ÑƒÑ‚Ğ¸ STDP ĞºĞµÑˆ")
    print(f"  {'â”€' * 50}\n")

    last_stats = {}

    while True:
        try:
            user_input = input("  Ğ¢Ğ¸: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\n  Ğ‘ÑƒĞ²Ğ°Ğ¹! ğŸ‘‹")
            break

        if not user_input:
            continue

        # â”€â”€ ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ¸ â”€â”€
        if user_input.startswith("/"):
            parts = user_input.split()
            cmd = parts[0].lower()

            if cmd == "/quit":
                print("  Ğ‘ÑƒĞ²Ğ°Ğ¹! ğŸ‘‹")
                break

            elif cmd == "/temp" and len(parts) > 1:
                try:
                    temperature = float(parts[1])
                    print(f"  [âš™] Temperature = {temperature}")
                except ValueError:
                    print(f"  [!] ĞĞµĞ²Ñ–Ñ€Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ")

            elif cmd == "/tokens" and len(parts) > 1:
                try:
                    max_tokens = int(parts[1])
                    print(f"  [âš™] Max tokens = {max_tokens}")
                except ValueError:
                    print(f"  [!] ĞĞµĞ²Ñ–Ñ€Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ")

            elif cmd == "/stdp":
                if len(parts) > 1 and parts[1].lower() in ("off", "0", "Ğ½Ñ–"):
                    stdp_enabled = False
                    print(f"  [âš™] STDP Ğ²Ğ¸Ğ¼ĞºĞ½ĞµĞ½Ğ¾")
                else:
                    stdp_enabled = True
                    print(f"  [âš™] STDP ÑƒĞ²Ñ–Ğ¼ĞºĞ½ĞµĞ½Ğ¾ â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ñ‡Ğ°Ñ‚Ñƒ!")

            elif cmd == "/rep" and len(parts) > 1:
                try:
                    rep_penalty = float(parts[1])
                    print(f"  [âš™] Repetition penalty = {rep_penalty}")
                    if rep_penalty > 2.0:
                        print(f"  [!] Ğ£Ğ²Ğ°Ğ³Ğ°: Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ > 2.0 Ğ¼Ğ¾Ğ¶Ğµ Ğ·Ğ»Ğ°Ğ¼Ğ°Ñ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ")
                except ValueError:
                    print(f"  [!] ĞĞµĞ²Ñ–Ñ€Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ")

            elif cmd == "/stats":
                if last_stats:
                    print(f"  [ğŸ“Š] ĞÑÑ‚Ğ°Ğ½Ğ½Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°:")
                    for k, v in last_stats.items():
                        print(f"       {k}: {v:.4f}")
                else:
                    print(f"  [!] Ğ©Ğµ Ğ½ĞµĞ¼Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ â€” Ğ½Ğ°Ğ¿Ğ¸ÑˆĞ¸ Ñ‰Ğ¾ÑÑŒ ÑĞ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ")

            elif cmd == "/reset":
                model._stdp_cache.clear()
                print(f"  [âš™] STDP ĞºĞµÑˆ ÑĞºĞ¸Ğ½ÑƒÑ‚Ğ¾")

            else:
                print(f"  [!] ĞĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ğ° ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°: {cmd}")

            continue

        # â”€â”€ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ â”€â”€
        t0 = time.time()

        response = generate(
            model, tokenizer, cfg,
            prompt=user_input,
            max_new_tokens=max_tokens,
            temperature=temperature,
            enable_stdp=stdp_enabled,
            repetition_penalty=rep_penalty,
            rep_window=rep_window,
        )

        elapsed = time.time() - t0

        print(f"\n  Nord: {response}")

        resp_tokens = len(tokenizer.encode(response, add_special_tokens=False))
        tps = resp_tokens / elapsed if elapsed > 0 else 0
        stdp_tag = " [STDP âœ“]" if stdp_enabled else ""
        rep_tag = f" [REP {rep_penalty}]" if rep_penalty > 1.0 else ""
        print(f"  [{resp_tokens} tok, {elapsed:.1f}s, {tps:.1f} tok/s{stdp_tag}{rep_tag}]\n")

        # Ğ—Ğ±ĞµÑ€ĞµĞ³Ñ‚Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
        with torch.no_grad(), torch.amp.autocast("cuda", enabled=(cfg.device == "cuda")):
            ids = tokenizer(user_input, return_tensors="pt",
                          truncation=True, max_length=cfg.max_seq_len).input_ids.to(cfg.device)
            _, last_stats = model(ids)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ENTRY POINT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print()
    print("â•" * 60)
    print("  âš¡ PROJECT NORD â€” Spiking Neural Network Chat v3.1")
    print("â•" * 60)

    default_model = os.path.join("D:", os.sep, "nord_model")
    print(f"\n  Ğ”Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ?")
    print(f"  (Enter = {default_model})")
    model_input = input("  Ğ¨Ğ»ÑÑ…: ").strip()
    model_dir = model_input if model_input else default_model

    if not Path(model_dir).exists():
        print(f"\n  [âœ—] ĞŸĞ°Ğ¿ĞºĞ° Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°: {model_dir}")
        print(f"  Ğ¡Ğ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ Ğ½Ğ°Ñ‚Ñ€ĞµĞ½ÑƒĞ¹:  python train_nord.py")
        sys.exit(1)

    model, tokenizer, cfg = load_model(model_dir)
    chat_loop(model, tokenizer, cfg)


if __name__ == "__main__":
    main()