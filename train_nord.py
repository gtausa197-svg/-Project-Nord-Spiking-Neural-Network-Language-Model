"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         PROJECT NORD â€” ĞšÑ€Ğ¾Ğº 2: ĞĞ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ SNN Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–                     â•‘
â•‘                                                                        â•‘
â•‘  ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸:                                                       â•‘
â•‘      python train_nord.py                                              â•‘
â•‘                                                                        â•‘
â•‘  Ğ’Ğ¾Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ñ”:                                                         â•‘
â•‘    1. Ğ”Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ (JSONL Ñ„Ğ°Ğ¹Ğ»)                                   â•‘
â•‘    2. ĞšÑƒĞ´Ğ¸ Ğ·Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ                                            â•‘
â•‘  Ğ† Ğ²ÑĞµ â€” Ğ´Ğ°Ğ»Ñ– Ñ‚Ñ€ĞµĞ½ÑƒÑ” Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾.                                      â•‘
â•‘                                                                        â•‘
â•‘  ĞœĞ¾Ğ¶Ğ½Ğ° Ğ·ÑƒĞ¿Ğ¸Ğ½Ğ¸Ñ‚Ğ¸ Ctrl+C Ñ– Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ñ‚Ğ¸ Ğ¿Ñ–Ğ·Ğ½Ñ–ÑˆĞµ â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµÑ‚ÑŒÑÑ.      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞŸĞ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾ Ğ²ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğ¸ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·:
    pip install torch transformers lmdb tqdm
"""

from __future__ import annotations

import json
import math
import os
import shutil
import struct
import sys
import time
from pathlib import Path
from typing import Optional

import torch
import torch.nn.functional as F
from torch.amp import autocast
from torch.utils.data import Dataset, DataLoader

from nord_core import NordConfig, NordModel


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¢ĞĞšĞ•ĞĞ†Ğ—ĞĞ¢ĞĞ 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class NordTokenizer:
    """ĞĞ±Ğ³Ğ¾Ñ€Ñ‚ĞºĞ° Llama-3.2 Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Project Nord."""

    def __init__(self, cfg: NordConfig):
        from transformers import AutoTokenizer

        print(f"  [*] Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ”Ğ¼Ğ¾ Llama-3.2 Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            cfg.tokenizer_id, trust_remote_code=True,
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        self.max_len = cfg.max_seq_len
        self.vocab_size = self.tokenizer.vocab_size
        if cfg.vocab_size < self.vocab_size:
            cfg.vocab_size = self.vocab_size

        print(f"  [âœ“] Ğ¢Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¹ (vocab={self.vocab_size:,})")

    def encode(self, text: str) -> torch.Tensor:
        enc = self.tokenizer(
            text, return_tensors="pt",
            max_length=self.max_len, truncation=True, padding="max_length",
        )
        return enc.input_ids

    def decode(self, ids) -> str:
        return self.tokenizer.decode(ids, skip_special_tokens=True)

    @property
    def pad_id(self) -> int:
        return self.tokenizer.pad_token_id


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LMDB Ğ”ĞĞ¢ĞĞ¡Ğ•Ğ¢ (on-disk, zero RAM)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LMDBDataset(Dataset):
    def __init__(self, db_path: str, max_seq_len: int):
        import lmdb
        self.db_path = db_path
        self.max_seq_len = max_seq_len
        self._env = None  # opened lazily â€” can't pickle lmdb.Environment on Windows

        # Read length once, then close
        env = lmdb.open(db_path, readonly=True, lock=False, readahead=False, meminit=False)
        with env.begin(write=False) as txn:
            raw = txn.get(b"__len__")
            self.length = struct.unpack("<Q", raw)[0]
        env.close()
        print(f"  [âœ“] LMDB: {self.length:,} Ğ·Ñ€Ğ°Ğ·ĞºÑ–Ğ²")

    def _get_env(self):
        """Lazy-open LMDB per worker process (safe for multiprocessing)."""
        if self._env is None:
            import lmdb
            self._env = lmdb.open(
                self.db_path, readonly=True, lock=False,
                readahead=True, meminit=False, max_readers=64,
            )
        return self._env

    def __len__(self): return self.length

    def __getitem__(self, idx):
        env = self._get_env()
        with env.begin(write=False) as txn:
            raw = txn.get(f"sample_{idx:010d}".encode())
        ids = torch.frombuffer(bytearray(raw), dtype=torch.int32).long()
        S = self.max_seq_len
        return ids[:S] if ids.shape[0] >= S else F.pad(ids, (0, S - ids.shape[0]))


def build_lmdb(jsonl_path: str, db_path: str, tokenizer: NordTokenizer,
               max_seq_len: int, map_size_gb: float = 50.0):
    """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ÑƒÑ” JSONL â†’ LMDB Ğ±Ğ°Ğ·Ñƒ (Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·)."""
    import lmdb
    from tqdm import tqdm

    print(f"\n  [*] Ğ‘ÑƒĞ´ÑƒÑ”Ğ¼Ğ¾ LMDB Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ¸Ñ…...")
    print(f"      Ğ¦Ğµ Ñ€Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ ĞĞ”Ğ˜Ğ Ñ€Ğ°Ğ· â€” Ğ¿Ğ¾Ñ‚Ñ–Ğ¼ Ñ‚Ñ€ĞµĞ½ÑƒÑ”ÑˆÑÑ Ğ· Ğ±Ğ°Ğ·Ğ¸ Ğ½ĞµÑĞºÑ–Ğ½Ñ‡ĞµĞ½Ğ½Ğ¾.")
    print(f"      Ğ”Ğ¶ĞµÑ€ĞµĞ»Ğ¾:  {jsonl_path}")
    print(f"      Ğ¦Ñ–Ğ»ÑŒ:     {db_path}")

    # ĞŸÑ–Ğ´Ñ€Ğ°Ñ…ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ€ÑĞ´ĞºĞ¸
    print(f"  [*] Ğ Ğ°Ñ…ÑƒÑ”Ğ¼Ğ¾ Ñ€ÑĞ´ĞºĞ¸...")
    with open(jsonl_path, "r", encoding="utf-8") as f:
        n_lines = sum(1 for _ in f)
    print(f"      Ğ—Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {n_lines:,} Ñ€ÑĞ´ĞºÑ–Ğ²")

    env = lmdb.open(db_path, map_size=int(map_size_gb * (1024 ** 3)))
    count = 0
    total_tokens = 0

    txn = env.begin(write=True)
    try:
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in tqdm(f, total=n_lines, desc="  Ğ¢Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ†Ñ–Ñ", unit=" doc"):
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    continue

                text = obj.get("text") or obj.get("content") or obj.get("passage", "")
                if len(text) < 30:
                    continue

                ids = tokenizer.encode(text).squeeze(0)
                non_pad = (ids != tokenizer.pad_id).sum().item()
                if non_pad < 10:
                    continue

                txn.put(f"sample_{count:010d}".encode(),
                        ids.to(torch.int32).numpy().tobytes())
                count += 1
                total_tokens += non_pad

                if count % 50_000 == 0:
                    txn.commit()
                    txn = env.begin(write=True)
                    print(f"      ... {count:,} Ğ·Ñ€Ğ°Ğ·ĞºÑ–Ğ², {total_tokens/1e6:.1f}M Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²")

        txn.put(b"__len__", struct.pack("<Q", count))
        txn.put(b"__total_tokens__", struct.pack("<Q", total_tokens))
        txn.commit()
    except BaseException:
        txn.abort()
        raise

    env.close()

    db_size = sum(f.stat().st_size for f in Path(db_path).rglob("*") if f.is_file())
    print(f"\n  [âœ“] LMDB Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ°!")
    print(f"      Ğ—Ñ€Ğ°Ğ·ĞºÑ–Ğ²:  {count:,}")
    print(f"      Ğ¢Ğ¾ĞºĞµĞ½Ñ–Ğ²:  {total_tokens:,} ({total_tokens/1e6:.1f}M)")
    print(f"      ĞĞ° Ğ´Ğ¸ÑĞºÑƒ:  {db_size / (1024**3):.2f} GB")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LR SCHEDULE
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_lr(step: int, cfg: NordConfig) -> float:
    if step < cfg.warmup_steps:
        return cfg.lr * (step + 1) / cfg.warmup_steps
    progress = min((step - cfg.warmup_steps) / max(1, cfg.max_steps - cfg.warmup_steps), 1.0)
    return cfg.min_lr + 0.5 * (1.0 + math.cos(math.pi * progress)) * (cfg.lr - cfg.min_lr)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ§Ğ•ĞšĞŸĞĞ†ĞĞ¢ ĞœĞ•ĞĞ•Ğ”Ğ–Ğ•Ğ 
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class CheckpointManager:
    def __init__(self, save_dir: str, keep_last: int = 5):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.keep_last = keep_last

    def save(self, model, optimizer, scaler, step, loss, cfg):
        path = self.save_dir / f"nord_step_{step:07d}.pt"
        torch.save({
            "step": step, "loss": loss,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scaler_state_dict": scaler.state_dict(),
            "config": {k: v for k, v in cfg.__dict__.items()
                       if not k.startswith("_") and k != "dtype"},
        }, path)

        latest = self.save_dir / "nord_latest.pt"
        if latest.exists():
            latest.unlink()
        shutil.copy2(path, latest)

        # Cleanup old
        ckpts = sorted(self.save_dir.glob("nord_step_*.pt"), key=lambda p: p.stat().st_mtime)
        for old in ckpts[:max(0, len(ckpts) - self.keep_last)]:
            old.unlink()

        print(f"  [ğŸ’¾] Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¾: {path.name} (loss={loss:.4f})")

    def load(self, model, optimizer, scaler, device) -> int:
        latest = self.save_dir / "nord_latest.pt"
        if not latest.exists():
            ckpts = sorted(self.save_dir.glob("nord_step_*.pt"))
            latest = ckpts[-1] if ckpts else None
        if latest is None:
            return 0

        print(f"  [*] Ğ’Ñ–Ğ´Ğ½Ğ¾Ğ²Ğ»ÑÑ”Ğ¼Ğ¾ Ğ·: {latest.name}")
        ckpt = torch.load(latest, map_location=device, weights_only=False)
        model.load_state_dict(ckpt["model_state_dict"])
        optimizer.load_state_dict(ckpt["optimizer_state_dict"])
        scaler.load_state_dict(ckpt["scaler_state_dict"])
        step = ckpt["step"]
        print(f"  [âœ“] Ğ’Ñ–Ğ´Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ĞºÑ€Ğ¾Ñ†Ñ– {step:,} (loss={ckpt.get('loss', '?')})")
        return step

    def save_final(self, model, cfg):
        """Ğ—Ğ±ĞµÑ€ĞµĞ³Ñ‚Ğ¸ Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ inference (Ğ¼ĞµĞ½ÑˆĞ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ»)."""
        path = self.save_dir / "nord_final.pt"
        torch.save({
            "model_state_dict": model.state_dict(),
            "config": {k: v for k, v in cfg.__dict__.items()
                       if not k.startswith("_") and k != "dtype"},
        }, path)
        print(f"  [â­] Ğ¤Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {path}")
        return path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ“ĞĞ›ĞĞ’ĞĞ Ğ¤Ğ£ĞĞšĞ¦Ğ†Ğ¯ ĞĞĞ’Ğ§ĞĞĞĞ¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def train(dataset_path: str, model_dir: str):
    # â”€â”€ ĞšĞ¾Ğ½Ñ„Ñ–Ğ³ â”€â”€
    cfg = NordConfig(
        device="cuda" if torch.cuda.is_available() else "cpu",
        dtype=torch.float16,
        d_model=512,
        n_heads=8,
        n_layers=6,
        d_ff=1024,
        T=8,
        T_slow=2,
        persistent_mem=False,  # shuffled batches â†’ no persistent state during training
        max_seq_len=512,
        batch_size=4,
        grad_accum=8,
        lr=5e-4,
        max_steps=100_000,
        save_every=1000,
        log_every=10,
    )

    print()
    print("â•" * 60)
    print("  PROJECT NORD v3 â€” ĞĞ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ SNN Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–")
    print("â•" * 60)
    print(f"  GPU:            {torch.cuda.get_device_name()}" if torch.cuda.is_available() else "  CPU mode")
    print(f"  ĞœĞ¾Ğ´ĞµĞ»ÑŒ:         d={cfg.d_model}, layers={cfg.n_layers}, T={cfg.T}+{cfg.T_slow}={cfg.T_total}")
    print(f"  Ğ•Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¸Ğ¹ Ğ±Ğ°Ñ‚Ñ‡: {cfg.batch_size} Ã— {cfg.grad_accum} = {cfg.batch_size * cfg.grad_accum}")
    print(f"  ĞšÑ€Ğ¾ĞºÑ–Ğ²:         {cfg.max_steps:,}")
    print(f"  Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚:        {dataset_path}")
    print(f"  ĞœĞ¾Ğ´ĞµĞ»ÑŒ â†’        {model_dir}")
    print()

    # â”€â”€ Ğ¢Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ â”€â”€
    tokenizer = NordTokenizer(cfg)

    # â”€â”€ LMDB Ğ±Ğ°Ğ·Ğ° (Ğ±ÑƒĞ´ÑƒÑ”Ñ‚ÑŒÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑĞºÑ‰Ğ¾ Ğ½Ğµ Ñ–ÑĞ½ÑƒÑ”) â”€â”€
    db_path = str(Path(dataset_path).with_suffix("")) + "_lmdb"
    if not Path(db_path).exists():
        build_lmdb(dataset_path, db_path, tokenizer, cfg.max_seq_len)

    dataset = LMDBDataset(db_path, cfg.max_seq_len)
    dataloader = DataLoader(
        dataset, batch_size=cfg.batch_size, shuffle=True,
        num_workers=2, pin_memory=True, drop_last=True, persistent_workers=True,
    )

    # â”€â”€ ĞœĞ¾Ğ´ĞµĞ»ÑŒ â”€â”€
    # ĞĞ• Ñ€Ğ¾Ğ±Ğ¸Ğ¼Ğ¾ .half() â€” autocast ÑĞ°Ğ¼ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ÑƒÑ” forward pass Ñƒ fp16,
    # Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸ Ğ·Ğ°Ğ»Ğ¸ÑˆĞ°ÑÑ‚ÑŒÑÑ fp32 Ğ´Ğ»Ñ ĞºĞ¾Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ñ— Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸ GradScaler
    print(f"\n  [*] Ğ‘ÑƒĞ´ÑƒÑ”Ğ¼Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ...")
    model = NordModel(cfg).to(cfg.device)
    print(f"  [âœ“] {model.count_params()}")

    # â”€â”€ Optimizer â”€â”€
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=cfg.lr,
        weight_decay=cfg.weight_decay, betas=(0.9, 0.95),
    )
    scaler = torch.amp.GradScaler("cuda", enabled=(cfg.dtype == torch.float16))

    # â”€â”€ Ğ§ĞµĞºĞ¿Ğ¾Ñ–Ğ½Ñ‚Ğ¸ (auto-resume) â”€â”€
    ckpt_mgr = CheckpointManager(model_dir)
    start_step = ckpt_mgr.load(model, optimizer, scaler, cfg.device)

    # â”€â”€ Ğ¢Ğ Ğ•ĞĞ£Ğ’ĞĞĞĞ¯ â”€â”€
    model.train()
    data_iter = iter(dataloader)
    running_loss = 0.0
    tokens_seen = 0
    t_start = time.time()

    print(f"\n  {'â”€' * 50}")
    print(f"  Ğ¡Ñ‚Ğ°Ñ€Ñ‚ Ğ· ĞºÑ€Ğ¾ĞºÑƒ {start_step:,}  |  {len(dataset):,} Ğ·Ñ€Ğ°Ğ·ĞºÑ–Ğ² Ğ² Ğ±Ğ°Ğ·Ñ–")
    print(f"  Ctrl+C = Ğ·ÑƒĞ¿Ğ¸Ğ½Ğ¸Ñ‚Ğ¸ (Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµÑ‚ÑŒÑÑ!)")
    print(f"  {'â”€' * 50}\n")

    try:
        for step in range(start_step, cfg.max_steps):
            accum_loss = 0.0
            stats = {}

            for _ in range(cfg.grad_accum):
                try:
                    input_ids = next(data_iter)
                except StopIteration:
                    data_iter = iter(dataloader)
                    input_ids = next(data_iter)

                input_ids = input_ids.to(cfg.device, non_blocking=True)

                with autocast(device_type="cuda", dtype=torch.float16,
                              enabled=(cfg.dtype == torch.float16)):
                    logits, stats = model(input_ids)

                    shift_logits = logits[:, :-1, :].contiguous()
                    shift_labels = input_ids[:, 1:].contiguous()

                    loss = F.cross_entropy(
                        shift_logits.reshape(-1, cfg.vocab_size),
                        shift_labels.reshape(-1),
                        ignore_index=tokenizer.pad_id,
                    ) / cfg.grad_accum

                scaler.scale(loss).backward()
                accum_loss += loss.item()
                tokens_seen += input_ids.numel()

            # Optimizer step
            scaler.unscale_(optimizer)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

            # LR schedule
            lr = get_lr(step, cfg)
            for pg in optimizer.param_groups:
                pg["lr"] = lr

            running_loss += accum_loss

            # Ğ›Ğ¾Ğ³
            if step % cfg.log_every == 0 and step > start_step:
                avg = running_loss / cfg.log_every
                elapsed = time.time() - t_start
                tps = tokens_seen / elapsed / 1000 if elapsed > 0 else 0
                sp = stats.get("sparsity", 0)

                print(
                    f"  ĞºÑ€Ğ¾Ğº {step:>7,} â”‚ "
                    f"loss {avg:.4f} â”‚ "
                    f"lr {lr:.1e} â”‚ "
                    f"grad {grad_norm:.1f} â”‚ "
                    f"sparsity {sp:.0%} â”‚ "
                    f"{tps:.1f}k tok/s"
                )
                running_loss = 0.0

            # Ğ—Ğ±ĞµÑ€ĞµĞ³Ñ‚Ğ¸
            if step > 0 and step % cfg.save_every == 0:
                ckpt_mgr.save(model, optimizer, scaler, step, accum_loss, cfg)

    except KeyboardInterrupt:
        print(f"\n\n  [â¸] Ğ—ÑƒĞ¿Ğ¸Ğ½ĞµĞ½Ğ¾ Ğ½Ğ° ĞºÑ€Ğ¾Ñ†Ñ– {step:,}")
        ckpt_mgr.save(model, optimizer, scaler, step, accum_loss, cfg)
        print(f"  Ğ©Ğ¾Ğ± Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ñ‚Ğ¸ â€” Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ·Ğ½Ğ¾Ğ²Ñƒ.")

    # Ğ—Ğ±ĞµÑ€ĞµĞ³Ñ‚Ğ¸ Ñ„Ñ–Ğ½Ğ°Ğ»ÑŒĞ½Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ñƒ
    ckpt_mgr.save_final(model, cfg)

    print(f"\n  {'â•' * 50}")
    print(f"  ĞĞ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
    print(f"  ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ° Ğ²: {model_dir}")
    print(f"  Ğ¢ĞµĞ¿ĞµÑ€ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ğ¹:  python chat.py")
    print(f"  {'â•' * 50}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ENTRY POINT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print("=" * 60)
    print("  PROJECT NORD â€” Ğ¢Ñ€ĞµĞ½ÑƒĞ²Ğ°Ğ½Ğ½Ñ SNN")
    print("=" * 60)

    # â”€â”€ Ğ—Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ñ‚Ğ¸ ÑˆĞ»ÑÑ… Ğ´Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ â”€â”€
    default_data = os.path.join("D:", os.sep, "nord_dataset", "train_data.jsonl")
    print(f"\n  Ğ”Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚? (JSONL Ñ„Ğ°Ğ¹Ğ»)")
    print(f"  (Enter = {default_data})")
    data_input = input("  Ğ¨Ğ»ÑÑ… Ğ´Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ: ").strip()
    dataset_path = data_input if data_input else default_data

    if not Path(dataset_path).exists():
        print(f"\n  [âœ—] Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {dataset_path}")
        print(f"  Ğ¡Ğ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸:  python download_data.py")
        sys.exit(1)

    # â”€â”€ Ğ—Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ñ‚Ğ¸ ĞºÑƒĞ´Ğ¸ Ğ·Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ â”€â”€
    default_model = os.path.join("D:", os.sep, "nord_model")
    print(f"\n  ĞšÑƒĞ´Ğ¸ Ğ·Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ?")
    print(f"  (Enter = {default_model})")
    model_input = input("  Ğ¨Ğ»ÑÑ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–: ").strip()
    model_dir = model_input if model_input else default_model

    # â”€â”€ ĞŸĞ¾Ñ—Ñ…Ğ°Ğ»Ğ¸ â”€â”€
    train(dataset_path, model_dir)


if __name__ == "__main__":
    main()
